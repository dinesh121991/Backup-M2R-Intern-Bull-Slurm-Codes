<!--#include virtual="header.txt"-->

<h1>What's New</h1>

<h2>Index</h2>
<ul>
<li><a href="#1411">Slurm Version 14.11, November 2014</a></li>
<li><a href="#1508">Slurm Version 15.08, August 2015</a></li>
<li><a href="#1605">Slurm Version 16.05 and beyond</a></li>
</ul>


<h2><a name="1411">Major Updates in Slurm Version 14.11</a></h2>
<p>Slurm Version 14.11 was released in November 2014.
Major enhancements include:
<ul>
<li>Added job array data structure and removed the array size restriction.</li>
<li>Performance for job array operations and database interactions are
    dramatically improved.</li>
<li>Added support for reserving CPUs and/or memory on a compute node for system
    use. This can largely eliminate system noise from applications.</li>
<li>Added support for allocation of generic resources by model type for
    heterogeneous systems (e.g. request a Kepler GPU, a Tesla GPU, or a GPU of
    any type).</li>
<li>Added support for non-consumable generic resources that are shared, but
    limited in number.</li>
<li>Added support for automatic job requeue policy based on exit value.</li>
<li>Added user options to set the CPU governor (OnDemand, Performance,
    PowerSave or UserSpace) in addition to being able to explicitly set the
    CPU frequency currently available.</li>
<li>Added support for an advanced reservation start time that remains constant
    relative to the current time.</li>
<li>Added reporting Slurm message traffic by user, type, count and time
    consumed.</li>
</ul>
</p>

<h2><a name="1508">Major Updates Planned for Slurm Version 15.08</a></h2>
<p>Slurm Version 15.08 is scheduled for release in August 2015.
Major enhancements to include:
<ul>
<!-- SchedMD led -->
<li>Add support for burst buffers, data storage available for before, during
    and/or after job computation in support of data staging, checkpoint,
    etc.</li>
<li>Convert charging from being based upon CPU time allocated to a more
    general <i>system billing unit</i>, which can be computed as a function of
    many different resources (e.g. CPU, memory, power, GPUs, etc.).</li>
<li>Improve recovery time for communication failures when large numbers of
    nodes fail simultaneously.</li>
<li>Permit disabling of swap space use.</li>
<!-- Universitat Jaume I & Universitat Politecnica de Valencia -->
<li>Add support for
<a href="http://slurm.schedmd.com/SUG14/remote_gpu.pdf">Remote CUDA (rCUDA)</a></li>
<!-- Intel led -->
<li>Add support for PMI Exascale (PMIx) for improved MPI scalability.</li>
<!-- Bull led -->
<li>Add support asymmetric resource allocation and MPMD programming. Multiple
    resource allocation specification (memory, CPUs, GPUs, etc.) will be
    supported in a single job allocation.</li>
<li>Add support for communication gateway nodes to improve scalability.</li>
<li>Add layouts framework, which will be the basis for further developments toward
    optimizing scheduling with respect to additional parameters such as temperature
    and power consumption.</li>
</ul>

<h2><a name="1605">Major Updates in Slurm Version 16.05 and beyond</a></h2>
<p> Detailed plans for release dates and contents of additional Slurm releases
have not been finalized. Anyone desiring to perform Slurm development should
notify <a href="mailto:slurm-dev@schedmd.com">slurm-dev@schedmd.com</a>
to coordinate activities. Future development plans includes:
<ul>
<li>Energy consumption added as a factor in fair-share scheduling.</li>
<li>Energy aware scheduling added with respect to power caps.</li>
<li>Improved support for GPU affinity with respect to CPUs and network
    resources.</li>
<li>Integration with
    <a href="http://en.wikipedia.org/wiki/FlexNet_Publisher">FLEXlm
    (Flexnet Publisher)</a> license management.</li>
<li>Distributed architecture to support the management of resources with Intel
    MIC processors.</li>
<li>IP communications over InfiniBand network for improved performance.</li>
<li>Fault-tolerance and jobs dynamic adaptation through communication protocol
    between Slurm , MPI libraries and the application.</li>
<li>Improved support for high-throughput computing (e.g. multiple slurmctld
    daemons on a single cluster).</li>
<li>Add Kerberos credential support including credential forwarding
    and refresh.</li>
<li>Improved support for provisioning and virtualization.</li> 
<li>Provide a web-based Slurm administration tool.</li>
</ul>
<p style="text-align:center;">Last modified 17 November 2014</p>

<!--#include virtual="footer.txt"-->
